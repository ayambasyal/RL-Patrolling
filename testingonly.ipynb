{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947283a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_0': 0, 'player_1': 1}\n"
     ]
    }
   ],
   "source": [
    "possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "agent_name_mapping = dict(\n",
    "    zip(possible_agents, list(range(len(possible_agents))))\n",
    ")\n",
    "\n",
    "print (agent_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d7c0cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ParallelEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomEnvironment\u001b[39;00m(\u001b[43mParallelEnv\u001b[49m):\n\u001b[1;32m      3\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_graph_environment_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# this is the graph\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ParallelEnv' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomEnvironment(ParallelEnv):\n",
    "    \n",
    "    metadata = {\n",
    "        \"name\": \"custom_graph_environment_v1\",\n",
    "    }\n",
    "    def __init__(self):\n",
    "        # this is the graph\n",
    "        self.g_env = nx.read_graphml('g1.gml')\n",
    "        self.g_no_node = len(self.g_env.nodes())\n",
    "        self.node_list = list(self.g_env.nodes())\n",
    "        \n",
    "        # A dictionary that relates each discrete value in the observation\n",
    "        # space to a node obtained from the graph\n",
    "        \n",
    "            # 1. Create empty dictionary\n",
    "        self.node_dict = {}\n",
    "        self.node_inv_dict = {}\n",
    "        self.timestep = 0\n",
    "        \n",
    "            # 2. relating the key to the value of the node\n",
    "        for key,value in enumerate(self.node_list):\n",
    "            self.node_dict[key] = value \n",
    "            \n",
    "            # 3. relating the value to the key ( used later on )\n",
    "        self.node_inv_dict = {value: key for key, value in self.node_dict.items()}\n",
    "        \n",
    "        # sets the maximum steps after which the program will terminate \n",
    "        self.max_steps = 100\n",
    "        self.step_now = 0\n",
    "        \n",
    "        \n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "        # {'player_0': 0, 'player_1': 1}\n",
    "\n",
    "        \n",
    "        self._action_spaces = {agent: Discrete(4) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(self.g_no_node**len(self.possible_agents)) for agent in self.possible_agents\n",
    "        }\n",
    "        \n",
    "        self.agents = [i for i in self.possible_agents]\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: None for agent in self.agents}\n",
    "        \n",
    "        # not utilized\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        \n",
    "                \n",
    "        # this is extra things for visualization you do not need to know\n",
    "        graph = self.g_env\n",
    "        position = list(graph.nodes())\n",
    "        position = [self.str_to_tuple(name) for name in position]\n",
    "        pos = dict(zip(graph.nodes(), position))\n",
    "        self.node_positions = pos\n",
    "        \n",
    "        # sets the current state of agents\n",
    "        # sets the thief to position 6\n",
    "        self.state['player_0'] = self.node_dict[6]\n",
    "        \n",
    "        # sets the police to position 9\n",
    "        self.state['player_1'] = self.node_dict[9]\n",
    "        \n",
    "        #forced by testing api\n",
    "        self.action_spaces = self._action_spaces\n",
    "        self.observation_spaces = self._observation_spaces\n",
    "        \n",
    "        self.terminations = {a:False for a in self.possible_agents}\n",
    "        \n",
    "        \n",
    "    def reset(self, seed=None, options=None,return_info=None):\n",
    "        self.timestep = 0\n",
    "        self.state = {agent: None for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.step_now = 0\n",
    "        self.agents = [i for i in self.possible_agents]\n",
    "        \n",
    "        # sets the thief to position 6\n",
    "        self.state['player_0'] = self.node_dict[6]\n",
    "        \n",
    "        # sets the police to position 9\n",
    "        self.state['player_1'] = self.node_dict[9]\n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        \n",
    "        # observation returns the next state of the agents\n",
    "        # for each action selected for the agent the observations sould be sent back\n",
    "        # the impletentation is bad change if possible\n",
    "        terminations = self.terminations \n",
    "        rewards = {\"player_0\":None, \"player_1\":None}\n",
    "        # taking the actions (input) \n",
    "        thief_action = actions[\"player_0\"]\n",
    "        police_action = actions[\"player_1\"]\n",
    "        \n",
    "        # current states\n",
    "        thief_state = self.state['player_0']\n",
    "        police_state = self.state['player_1']\n",
    "        \n",
    "        # possible_thief are the possible places the thief can go \n",
    "        possible_thief = [i for i in self.g_env.neighbors(thief_state)]\n",
    "        possible_police = [i for i in self.g_env.neighbors(police_state)]\n",
    "        \n",
    "        \n",
    "        # movement of the thief and the police according to the action and their rewards       \n",
    "        for agent in self.possible_agents:\n",
    "            temp_neighbours = []\n",
    "            for neighbour in self.g_env.neighbors(self.state[agent]):\n",
    "                temp_neighbours.append(neighbour)\n",
    "    \n",
    "            if actions[agent] < len([i for i in self.g_env.neighbors(self.state[agent])]):\n",
    "                rewards[agent] = -6;\n",
    "                self.state[agent] = temp_neighbours[actions[agent]]\n",
    "                self.step_now += 1\n",
    "\n",
    "            elif actions[agent] == len([i for i in self.g_env.neighbors(self.state[agent])]):\n",
    "                rewards[agent] = -3;\n",
    "#                 print('no move action')\n",
    "                self.step_now += 1\n",
    "            else:\n",
    "                rewards[agent] = -10;\n",
    "        \n",
    "            \n",
    "        # updating states of thief and police\n",
    "        thief_state = self.state['player_0']\n",
    "        police_state = self.state['player_1']\n",
    "        \n",
    "        \n",
    "        # reward for catching the thief and         \n",
    "            # checking for terminations\n",
    "            # nice one  \n",
    "        if(police_state == thief_state):\n",
    "            terminations = {a:True for a in self.possible_agents}\n",
    "#             print('reward provided')\n",
    "            rewards['player_1'] += 70\n",
    "\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.possible_agents}\n",
    "        observations = {a: self.state[a] for a in self.possible_agents}\n",
    "        truncations = {a: False for a in self.possible_agents}\n",
    "        \n",
    "        # Check truncation conditions (overwrites termination conditions)\n",
    "        if self.timestep > 100:\n",
    "            rewards = {\"player_0\": 0, \"player_1\": 0}\n",
    "            truncations = {\"player_0\": True, \"player_1\": True}\n",
    "            self.agents = []\n",
    "        self.timestep += 1\n",
    "        \n",
    "        \n",
    "        temp_agents = [agent for agent in self.agents]\n",
    "        \n",
    "        # remove terminated agents\n",
    "        for agent in self.agents:\n",
    "            if terminations[agent] == True :\n",
    "                temp_agents.remove(agent)\n",
    "        self.agents = temp_agents\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    # only works for 2 players\n",
    "    def temp_render(self,episode):\n",
    "        \n",
    "        nx.draw(self.g_env, self.node_positions,node_size=300)\n",
    "        nx.draw_networkx_labels(self.g_env, self.node_positions,labels = self.node_inv_dict,font_color='black' )\n",
    "        \n",
    "        x1,y1 = self.node_positions[str(self.state['player_0'])]\n",
    "        x2,y2 = self.node_positions[str(self.state['player_1'])]\n",
    "        \n",
    "        filename = f\"images/Multi_{episode}_{self.timestep}.png\"\n",
    "        \n",
    "        \n",
    "        plt.scatter(x2, y2, s=550, c='yellow')\n",
    "        plt.scatter(x1, y1, s=450, c='red')    \n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "    \n",
    "    def str_to_tuple(self,string):\n",
    "        return tuple(float(x) for x in string.strip('()').split(','))\n",
    "    \n",
    "    def possible_move_range(self,player):\n",
    "        return len(list(self.g_env.neighbors(self.state[player])))\n",
    "    def show_node_num(self):\n",
    "        node = []\n",
    "        for agent in self.possible_agents:\n",
    "            node.append([agent,self.node_inv_dict[self.state[agent]]])\n",
    "        return node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763504ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thief_0': None, 'police_0': None, 'police_1': None, 'police_2': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_of_thieves = 1\n",
    "no_of_police = 3 \n",
    "possible_thieves = ['thief_'+str(r) for r in range (no_of_thieves)]\n",
    "possible_police = ['police_'+str(r) for r in range (no_of_police)]\n",
    "possible_agents = possible_thieves + possible_police\n",
    "\n",
    "rewards = {}\n",
    "\n",
    "for agent in possible_agents:\n",
    "            rewards[agent] = None\n",
    "        \n",
    "print( rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875fe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
