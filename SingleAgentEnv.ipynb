{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbe80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.spatial import Voronoi\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import random\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Graph,Discrete,Box\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d2c76",
   "metadata": {},
   "source": [
    "## This section creates an environment from the graph created by the Graph Creator\n",
    "This environment is somewhat similar to frozenlake,but without the holes, it gets is 20 reward points when it reaches the destination and -2 for any other step taken that does not give an output\n",
    "> As the action space takes value (0,1,2) and there might be less than 3 commitable actions so, sometimes when an action that cannot taken, no reward is given and the no of steps is not added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # max 3 possible actions, go to another node (for now)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        # this is the graph\n",
    "        self.g_env = nx.read_graphml('g1.gml')\n",
    "        self.g_no_node = len(self.g_env.nodes())\n",
    "        self.node_list = list(self.g_env.nodes())\n",
    "        \n",
    "         # observation space\n",
    "        self.observation_space = Discrete(self.g_no_node-1)\n",
    "        \n",
    "        \n",
    "        # A dictionary that relates each discrete value in the observation\n",
    "        # space to a node obtained from the graph\n",
    "        \n",
    "            # 1. Create empty dictionary\n",
    "        self.node_dict = {}\n",
    "        self.node_inv_dict = {}\n",
    "        \n",
    "            # 2. relating the key to the value of the node\n",
    "        for key,value in enumerate(self.node_list):\n",
    "            self.node_dict[key] = value \n",
    "            \n",
    "            # 3. relating the value to the key ( used later on )\n",
    "        self.node_inv_dict = {value: key for key, value in self.node_dict.items()}\n",
    "        \n",
    "            # 4. to determine the reward obtained in the given step\n",
    "            \n",
    "        \n",
    "        # starting state of the graph\n",
    "        self.state = self.node_dict[6]\n",
    "        \n",
    "#         print(self.node_dict)\n",
    "        \n",
    "        # sets the maximum steps after which the program will terminate \n",
    "        self.max_steps = 100\n",
    "        self.step_now = 0\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # extra dont know why when self.state exists \n",
    "        self.current_state = self.node_dict[6]\n",
    "        \n",
    "        \n",
    "        # this is extra things for visualization you do not need to know\n",
    "        graph = self.g_env\n",
    "        \n",
    "        position = list(graph.nodes())\n",
    "#         pprint.pprint(position)\n",
    "        \n",
    "        position = [self.str_to_tuple(name) for name in position]\n",
    "#         pprint.pprint(position)\n",
    "        pos = dict(zip(graph.nodes(), position))\n",
    "#         pprint.pprint(pos)\n",
    "        \n",
    "        self.node_positions = pos\n",
    "\n",
    "    def step(self,action_1):\n",
    "        state = self.state\n",
    "        possible = [i for i in self.g_env.neighbors(state)]\n",
    "        if action_1<len(possible):\n",
    "            state = possible[action_1]\n",
    "            self.step_now = self.step_now + 1\n",
    "            if state == self.node_dict[2]:\n",
    "                reward = 20\n",
    "            else:\n",
    "                reward = -2 \n",
    "        else:\n",
    "            reward = 0\n",
    "        if ((self.step_now < self.max_steps) and (state != self.node_dict[2])):\n",
    "            terminal = False\n",
    "        else:\n",
    "            terminal = True\n",
    "#             print(\"terminating episode\")\n",
    "        self.state = state\n",
    "        return self.state,reward,terminal,{}\n",
    "    \n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    def reset(self):\n",
    "        self.state = self.node_dict[6]\n",
    "        self.step_now = 0\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    # extra funcitons for testing and visualization\n",
    "    \n",
    "    def show_step(self):\n",
    "        return (self.step_now)\n",
    "    \n",
    "    def temp_render(self,episode):\n",
    "        nx.draw(self.g_env, self.node_positions,node_size=200)\n",
    "        nx.draw_networkx_labels(self.g_env, self.node_positions,labels = self.node_inv_dict,font_color='black' )\n",
    "#         print(self.state)\n",
    "        x,y = self.node_positions[str(self.state)]\n",
    "#         print(x,y)\n",
    "        filename = f\"images/plot_{episode}_{self.step_now}.png\"\n",
    "        plt.scatter(x, y, s=350, c='red')\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "    \n",
    "    def str_to_tuple(self,string):\n",
    "        return tuple(float(x) for x in string.strip('()').split(','))\n",
    "    \n",
    "    def show_no_of_nodes(self):\n",
    "        value = self.g_no_node\n",
    "        return value\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e637be",
   "metadata": {},
   "source": [
    "### First Steps for Implementing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a environment into the env variable\n",
    "# so that env now acts as the \n",
    "env = GraphEnv()\n",
    "\n",
    "# Renders the environment state that we created\n",
    "# the 1000 here is the episode number passed so that the output can be tracked according to the environment\n",
    "env.temp_render(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs 2 episodes of set of random actions in the environment until the terminating state is reached\n",
    "episodes = 2\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "#         env.temp_render(episode)\n",
    "        action = env.action_space.sample()\n",
    "        n_state,reward,done,info = env.step(action)\n",
    "        if (done == True):\n",
    "            print('terminating episode = {}'.format(env.show_step()))\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {} \\n'.format(episode,score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
