{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils.env import ParallelEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62550102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7119c",
   "metadata": {},
   "source": [
    "## One thief and one Police environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvironment(ParallelEnv):\n",
    "    \n",
    "    metadata = {\n",
    "        \"name\": \"custom_graph_environment_v1\",\n",
    "    }\n",
    "    def __init__(self):\n",
    "        # this is the graph\n",
    "        self.g_env = nx.read_graphml('g1.gml')\n",
    "        self.g_no_node = len(self.g_env.nodes())\n",
    "        self.node_list = list(self.g_env.nodes())\n",
    "        \n",
    "        # A dictionary that relates each discrete value in the observation\n",
    "        # space to a node obtained from the graph\n",
    "        \n",
    "            # 1. Create empty dictionary\n",
    "        self.node_dict = {}\n",
    "        self.node_inv_dict = {}\n",
    "        \n",
    "            # 2. relating the key to the value of the node\n",
    "        for key,value in enumerate(self.node_list):\n",
    "            self.node_dict[key] = value \n",
    "            \n",
    "            # 3. relating the value to the key ( used later on )\n",
    "        self.node_inv_dict = {value: key for key, value in self.node_dict.items()}\n",
    "        \n",
    "        # sets the maximum steps after which the program will terminate \n",
    "        self.max_steps = 100\n",
    "        self.step_now = 0\n",
    "        \n",
    "        \n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "        # {'player_0': 0, 'player_1': 1}\n",
    "\n",
    "        \n",
    "        self._action_spaces = {agent: Discrete(4) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(self.g_no_node**len(self.possible_agents)) for agent in self.possible_agents\n",
    "        }\n",
    "        \n",
    "        self.agents = self.possible_agents\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: None for agent in self.agents}\n",
    "        \n",
    "        # not utilized\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        \n",
    "                \n",
    "        # this is extra things for visualization you do not need to know\n",
    "        graph = self.g_env\n",
    "        position = list(graph.nodes())\n",
    "        position = [self.str_to_tuple(name) for name in position]\n",
    "        pos = dict(zip(graph.nodes(), position))\n",
    "        self.node_positions = pos\n",
    "        \n",
    "        # sets the current state of agents\n",
    "        # sets the thief to position 6\n",
    "        self.state['player_0'] = self.node_dict[6]\n",
    "        \n",
    "        # sets the police to position 9\n",
    "        self.state['player_1'] = self.node_dict[9]\n",
    "        \n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.timestep = None\n",
    "        self.state = {agent: None for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.step_now = 0\n",
    "        \n",
    "        # sets the thief to position 6\n",
    "        self.state['player_0'] = self.node_dict[6]\n",
    "        \n",
    "        # sets the police to position 9\n",
    "        self.state['player_1'] = self.node_dict[9]\n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        \n",
    "        # observation returns the next state of the agents\n",
    "        # for each action selected for the agent the observations sould be sent back\n",
    "        # the impletentation is bad change if possible\n",
    "        terminations = False\n",
    "        rewards = {\"player_0\":None, \"player_1\":None}\n",
    "        # taking the actions (input) \n",
    "        thief_action = actions[\"player_0\"]\n",
    "        police_action = actions[\"player_1\"]\n",
    "        \n",
    "        # current states\n",
    "        thief_state = self.state['player_0']\n",
    "        police_state = self.state['player_1']\n",
    "        \n",
    "        # possible_thief are the possible places the thief can go \n",
    "        possible_thief = [i for i in self.g_env.neighbors(thief_state)]\n",
    "        possible_police = [i for i in self.g_env.neighbors(police_state)]\n",
    "        \n",
    "        \n",
    "        # movement of the thief and the police according to the action and their rewards       \n",
    "        for agent in self.possible_agents:\n",
    "            temp_neighbours = []\n",
    "            for neighbour in self.g_env.neighbors(self.state[agent]):\n",
    "                temp_neighbours.append(neighbour)\n",
    "    \n",
    "            if actions[agent] < len([i for i in self.g_env.neighbors(self.state[agent])]):\n",
    "                rewards[agent] = -2;\n",
    "                self.state[agent] = temp_neighbours[actions[agent]]\n",
    "                self.step_now += 1\n",
    "\n",
    "            elif actions[agent] == len([i for i in self.g_env.neighbors(self.state[agent])]):\n",
    "                rewards[agent] = -1;\n",
    "                self.step_now += 1\n",
    "            else:\n",
    "                rewards[agent] = -10;\n",
    "                \n",
    "            # checking for terminations\n",
    "            # nice one\n",
    "#             for index in possible_thief:\n",
    "#                 for i in range(len(possible_police)):\n",
    "#                     if(index == possible_police[i]):\n",
    "#                         terminations = True;    \n",
    "            if(police_state == thief_state):\n",
    "                terminations = True\n",
    "            for index in possible_thief:\n",
    "                if(index == police_state):\n",
    "                    terminations = True;\n",
    "        \n",
    "        # extra reward for thief for running away\n",
    "        if (self.step_now>7):\n",
    "                rewards['player_0'] += 20\n",
    "                rewards['player_1'] -= 20\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "        observations = {a: self.state[a] for a in self.agents}\n",
    "        truncations = {a: None for a in self.agents}\n",
    "        \n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    # only works for 2 players\n",
    "    def temp_render(self,episode):\n",
    "        \n",
    "        nx.draw(self.g_env, self.node_positions,node_size=300)\n",
    "        nx.draw_networkx_labels(self.g_env, self.node_positions,labels = self.node_inv_dict,font_color='black' )\n",
    "        \n",
    "        x1,y1 = self.node_positions[str(self.state['player_0'])]\n",
    "        x2,y2 = self.node_positions[str(self.state['player_1'])]\n",
    "        \n",
    "        filename = f\"images/Multi_{episode}_{self.step_now}.png\"\n",
    "        \n",
    "        plt.scatter(x2, y2, s=550, c='purple')\n",
    "        plt.scatter(x1, y1, s=450, c='red')    \n",
    "        \n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "    \n",
    "    def str_to_tuple(self,string):\n",
    "        return tuple(float(x) for x in string.strip('()').split(','))\n",
    "    \n",
    "    def possible_move_range(self,player):\n",
    "        return len(list(self.g_env.neighbors(self.state[player])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf24432",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment()\n",
    "episodes = 1\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    terminations = False\n",
    "    score = {a:0 for a in env.possible_agents}\n",
    "    print(state)\n",
    "    while not terminations:\n",
    "        n0 = np.random.randint(0,env.possible_move_range('player_0')+1)\n",
    "        n1 = np.random.randint(0,env.possible_move_range('player_1')+1)\n",
    "        print(n0)\n",
    "        print(n1)\n",
    "        env.temp_render(episode)\n",
    "        action = {'player_0':n0,'player_1':n1}\n",
    "        observations, rewards, terminations, truncations, infos = env.step(action)\n",
    "        if (terminations == True):\n",
    "            print('terminating episode = {}'.format(env.step_now))\n",
    "        for agent in env.possible_agents:\n",
    "            score[agent] += rewards[agent]\n",
    "    print('Episode: {} Score: {} \\n'.format(episode,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e748bf1",
   "metadata": {},
   "source": [
    "## Example of implementation of rock paper scissor game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class raw_env(AECEnv):\n",
    "    \"\"\"\n",
    "    The metadata holds environment constants. From gymnasium, we inherit the \"render_modes\",\n",
    "    metadata which specifies which modes can be put into the render() method.\n",
    "    At least human mode should be supported.\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and\n",
    "         should define the following attributes:\n",
    "        - possible_agents\n",
    "        - action_spaces\n",
    "        - observation_spaces\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(4) for agen t in self.possible_agents\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # this cache ensures that same space object is returned for the same agent\n",
    "    # allows action space seeding to work as expected\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Observe should return the observation of the specified agent. This function\n",
    "        should return a sane observation (though not necessarily the most up to date possible)\n",
    "        at any time after reset() is called.\n",
    "        \"\"\"\n",
    "        # observation of one agent is the previous state of the other\n",
    "        return np.array(self.observations[agent])\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the following attributes\n",
    "        - agents\n",
    "        - rewards\n",
    "        - _cumulative_rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection\n",
    "        And must set up the environment so that render(), step(), and observe()\n",
    "        can be called without issues.\n",
    "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: NONE for agent in self.agents}\n",
    "        self.observations = {agent: NONE for agent in self.agents}\n",
    "        self.num_moves = 0\n",
    "        \"\"\"\n",
    "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \"\"\"\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for the current agent (specified by\n",
    "        agent_selection) and needs to update\n",
    "        - rewards\n",
    "        - _cumulative_rewards (accumulating the rewards)\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection (to the next agent)\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            # handles stepping an agent which is already dead\n",
    "            # accepts a None action for the one agent, and moves the agent_selection to\n",
    "            # the next dead agent,  or if there are no more dead agents, to the next live agent\n",
    "            self._was_dead_step(action)\n",
    "            return\n",
    "\n",
    "        agent = self.agent_selection\n",
    "\n",
    "        # the agent which stepped last had its _cumulative_rewards accounted for\n",
    "        # (because it was returned by last()), so the _cumulative_rewards for this\n",
    "        # agent should start again at 0\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "\n",
    "        # stores action of current agent\n",
    "        self.state[self.agent_selection] = action\n",
    "\n",
    "        # collect reward if it is the last agent to act\n",
    "        if self._agent_selector.is_last():\n",
    "            # rewards for all agents are placed in the .rewards dictionary\n",
    "            self.rewards[self.agents[0]], self.rewards[self.agents[1]] = REWARD_MAP[\n",
    "                (self.state[self.agents[0]], self.state[self.agents[1]])\n",
    "            ]\n",
    "\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "\n",
    "            # observe the current state\n",
    "            for i in self.agents:\n",
    "                self.observations[i] = self.state[\n",
    "                    self.agents[1 - self.agent_name_mapping[i]]\n",
    "                ]\n",
    "        else:\n",
    "            # necessary so that observe() returns a reasonable observation at all times.\n",
    "            self.state[self.agents[1 - self.agent_name_mapping[agent]]] = NONE\n",
    "            # no rewards are allocated until both players give an action\n",
    "            self._clear_rewards()\n",
    "\n",
    "        # selects the next agent.\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        # Adds .rewards to ._cumulative_rewards\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reference prisoner guard environment\n",
    "\n",
    "# import functools\n",
    "# import random\n",
    "# from copy import copy\n",
    "\n",
    "# import numpy as np\n",
    "# from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "\n",
    "# from pettingzoo.utils.env import ParallelEnv\n",
    "\n",
    "# class CustomEnvironment(ParallelEnv):\n",
    "#     metadata = {\n",
    "#         \"name\": \"custom_environment_v0\",\n",
    "#     }\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.escape_y = None\n",
    "#         self.escape_x = None\n",
    "#         self.guard_y = None\n",
    "#         self.guard_x = None\n",
    "#         self.prisoner_y = None\n",
    "#         self.prisoner_x = None\n",
    "#         self.timestep = None\n",
    "#         self.possible_agents = [\"prisoner\", \"guard\"]\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         self.agents = copy(self.possible_agents)\n",
    "#         self.timestep = 0\n",
    "\n",
    "#         self.prisoner_x = 0\n",
    "#         self.prisoner_y = 0\n",
    "\n",
    "#         self.guard_x = 7\n",
    "#         self.guard_y = 7\n",
    "\n",
    "#         self.escape_x = random.randint(2, 5)\n",
    "#         self.escape_y = random.randint(2, 5)\n",
    "\n",
    "#         observations = {\n",
    "#             a: (\n",
    "#                 self.prisoner_x + 7 * self.prisoner_y,\n",
    "#                 self.guard_x + 7 * self.guard_y,\n",
    "#                 self.escape_x + 7 * self.escape_y,\n",
    "#             )\n",
    "#             for a in self.agents\n",
    "#         }\n",
    "#         return observations\n",
    "\n",
    "#     def step(self, actions):\n",
    "#         # Execute actions\n",
    "#         prisoner_action = actions[\"prisoner\"]\n",
    "#         guard_action = actions[\"guard\"]\n",
    "\n",
    "#         if prisoner_action == 0 and self.prisoner_x > 0:\n",
    "#             self.prisoner_x -= 1\n",
    "#         elif prisoner_action == 1 and self.prisoner_x < 6:\n",
    "#             self.prisoner_x += 1\n",
    "#         elif prisoner_action == 2 and self.prisoner_y > 0:\n",
    "#             self.prisoner_y -= 1\n",
    "#         elif prisoner_action == 3 and self.prisoner_y < 6:\n",
    "#             self.prisoner_y += 1\n",
    "\n",
    "#         if guard_action == 0 and self.guard_x > 0:\n",
    "#             self.guard_x -= 1\n",
    "#         elif guard_action == 1 and self.guard_x < 6:\n",
    "#             self.guard_x += 1\n",
    "#         elif guard_action == 2 and self.guard_y > 0:\n",
    "#             self.guard_y -= 1\n",
    "#         elif guard_action == 3 and self.guard_y < 6:\n",
    "#             self.guard_y += 1\n",
    "\n",
    "#         # Check termination conditions\n",
    "#         terminations = {a: False for a in self.agents}\n",
    "#         rewards = {a: 0 for a in self.agents}\n",
    "#         if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "#             rewards = {\"prisoner\": -1, \"guard\": 1}\n",
    "#             terminations = {a: True for a in self.agents}\n",
    "\n",
    "#         elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "#             rewards = {\"prisoner\": 1, \"guard\": -1}\n",
    "#             terminations = {a: True for a in self.agents}\n",
    "\n",
    "#         # Check truncation conditions (overwrites termination conditions)\n",
    "#         truncations = {a: False for a in self.agents}\n",
    "#         if self.timestep > 100:\n",
    "#             rewards = {\"prisoner\": 0, \"guard\": 0}\n",
    "#             truncations = {\"prisoner\": True, \"guard\": True}\n",
    "#             self.agents = []\n",
    "#         self.timestep += 1\n",
    "\n",
    "#         # Get observations\n",
    "#         observations = {\n",
    "#             a: (\n",
    "#                 self.prisoner_x + 7 * self.prisoner_y,\n",
    "#                 self.guard_x + 7 * self.guard_y,\n",
    "#                 self.escape_x + 7 * self.escape_y,\n",
    "#             )\n",
    "#             for a in self.agents\n",
    "#         }\n",
    "\n",
    "#         # Get dummy infos (not used in this example)\n",
    "#         infos = {a: {} for a in self.agents}\n",
    "\n",
    "#         return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "#     def render(self):\n",
    "#         grid = np.zeros((7, 7))\n",
    "#         grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "#         grid[self.guard_y, self.guard_x] = \"G\"\n",
    "#         grid[self.escape_y, self.escape_x] = \"E\"\n",
    "#         print(f\"{grid} \\n\")\n",
    "\n",
    "#     @functools.lru_cache(maxsize=None)\n",
    "#     def observation_space(self, agent):\n",
    "#         return MultiDiscrete([7 * 7 - 1] * 3)\n",
    "\n",
    "#     @functools.lru_cache(maxsize=None)\n",
    "#     def action_space(self, agent):\n",
    "#         return Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27655714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6baa71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
